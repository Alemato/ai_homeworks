{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c77a157ce5e76d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Homework 1\n",
    "**Student:** Alessandro Mattei\n",
    "\n",
    "**Matricola:** 295441\n",
    "\n",
    "**Email:** alessandro.mattei1@student.univaq.it\n",
    "\n",
    "## Introduzione\n",
    "Nel Homework 2 è stato richiesto di modificare l'algoritmo MinMaxAlphaBetaPruning in tre modi:\n",
    "- Utilizzando l'H0 cutoff\n",
    "- Utilizzando l'Hl cutoff\n",
    "- Utilizzando un regressore non lineare\n",
    "\n",
    "Tutto questo per verificare se con queste modifiche si migliorasse l'algoritmo MinMax Alpha Beta Pruning e se si riuscisse ad arrivare a profondità maggiori della prima sperimentazione effettuata nel primo Homework. \n",
    "\n",
    "\n",
    "Nel gioco troviamo una classe Agent generica che si può utilizzare per qualsiasi gioco che prende in input un **search_algorithm** e un **initial_state**.\n",
    "L'Agente tramite la funzione **do_action** ritornerà un nuovo stato di gioco tramite l'utilizzo del **search_algorithm** e di un euristica.\n",
    "L'Agente dopo n iterazioni risolverà entrami i giochi proposti.\n",
    "\n",
    "Più avanti verrà mostrato e descritto l'implementazione delle classi e delle funzioni scritte in Python utilizzate:\n",
    "- AlessandroMattei_ChessGame.AIhw2.ipynb\n",
    "\n",
    "Nella fase finale viene mostrata e descritta una analisi statistica dei risultati delle varie sperimentazioni eseguite con istanze di diversi algoritmi di ricerca o configurazioni di essi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8448953620641b24",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Implementazione\n",
    "## Agente - Agent Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9e1287d395d237",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "```python\n",
    "class Agent:\n",
    "    \"\"\"\n",
    "    Represents an agent that can act based on a given search algorithm and its current view of the world.\n",
    "\n",
    "    Attributes:\n",
    "        search_algorithm: A search algorithm that the agent uses to make decisions.\n",
    "        view: The agent's current view of the world.\n",
    "        old_view: The agent's previous view of the world.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, , initial_state):\n",
    "        \"\"\"\n",
    "        Initializes the Agent with a search algorithm and an initial state.\n",
    "\n",
    "        :param search_algorithm: The search algorithm to be used by the agent.\n",
    "        :param initial_state: The initial state of the world as perceived by the agent.\n",
    "        \"\"\"\n",
    "        self.search_algorithm = search_algorithm\n",
    "        self.view = initial_state\n",
    "        self.old_view = None\n",
    "\n",
    "    def do_action(self, current_state_world):\n",
    "        \"\"\"\n",
    "        Updates the agent's view based on the current state of the world and the search algorithm.\n",
    "        :param current_state_world: The current state of the world.\n",
    "        :return: The updated view of the agent.\n",
    "        \"\"\"\n",
    "        self.view = self.search_algorithm.search(current_state_world)\n",
    "        self.old_view = current_state_world\n",
    "        return self.view\n",
    "```\n",
    "La classe agente è indipendente dal tipo di gioco o problema che si vuole risolvere.\n",
    "Si occupa di richiamare l’algoritmo di ricerca (**search_algorithm**), tramite il metodo **do_action**, il quale ritornerà uno stato successivo passando come parametro lo stato attuale.\n",
    "L’agente viene chiamato dalla funzione “main” ad ogni mossa e restituisce lo stato successivo migliore (secondo l'euristica scelta) che verrà a sua volta impiegato nel successivo ciclo come parametro fino alla fine dell'esecuzione.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Algoritmi di Ricerca Implementati"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "357ad4de5b134851"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MinMaxAlpha-BetaPruning\n",
    "L'algoritmo MinMax con Alpha-Beta Pruning è una ottimizzazione dell'algoritmo MinMax tradizionale, utilizzato nei giochi a due giocatori come Scacchi.\n",
    "\n",
    "Il suo obiettivo principale è ridurre il numero di nodi valutati nell'albero di ricerca, \"tagliando\" rami che non influenzeranno la decisione finale.\n",
    "Questo permette di esplorare alberi più profondi in meno tempo, migliorando le prestazioni.\n",
    "\n",
    "La strategia si basa sull'utilizzo di due parametri chiave: alpha e beta. Immaginando che il giocatore 1 sia quello che mira a massimizzare il punteggio e il giocatore 2 a minimizzarlo:\n",
    "- Alpha simbolizza il punteggio minimo che il giocatore 1 può assicurarsi nella posizione attuale. Sebbene parta dal valore peggiore per il giocatore 1, si aggiorna costantemente in base alla mossa più vantaggiosa che il giocatore 1 potrebbe fare.\n",
    "- Beta, al contrario, rappresenta il punteggio ottimale che il giocatore 2 può aspirare a ottenere. Anch'esso inizia dal valore peggiore per il giocatore 2, ma si rinnova considerando la mossa migliore individuata per il giocatore 2 fino a quel punto.\n",
    "\n",
    "La dinamica procede seguendo la struttura della ricerca MinMax, con aggiornamenti continui di alpha e beta ad ogni nodo esaminato.\n",
    "Se, in una certa fase dell'analisi, alpha dovesse superare beta, l'esplorazione del ramo attuale viene interrotta, permettendo all'algoritmo di concentrarsi su percorsi alternativi. Così facendo, l'intero sotto-albero legato a nodi in cui i valori di alpha e beta si \"incrociano\" viene bypassato, ottimizzando l'efficienza dell'analisi.\n",
    "\n",
    "L’algoritmo per essere istanziato ha bisogno dell’euristica, del gioco e della profondità alla quale deve lavorare. La variabile *eval_count* è una variabile che conta il numero degli stati valutati utile per stampare i risultati e la variabile *prune_count* è una variabile che ci dice quanti elementi sono stati potati\n",
    "\n",
    "```python\n",
    "def __init__(self, game, heuristic, max_depth=1):\n",
    "    \"\"\"\n",
    "    Initializes an instance of the MinMaxAlphaBetaPruning class.\n",
    "    :param game: The game for which the search is performed.\n",
    "    :param heuristic: The heuristic to evaluate the game states.\n",
    "    :param max_depth: Maximum depth of the search. Default is 1.\n",
    "    \"\"\"\n",
    "    self.game = game\n",
    "    self.heuristic = heuristic\n",
    "    self.max_depth = max_depth\n",
    "    self.prune_count = 0\n",
    "    self.eval_count = 0\n",
    "```\n",
    "Come si può vedere dal file *AlessandroMattei_ChessGame.AIhw1.ipynb*, nel quale è contenuta l'intera implementazione, possiamo notare la presenza di tre metodi **__minmax_alpha_beta()**, **evaluate()**, **pick()** e **search()**.\n",
    "\n",
    "Di seguito possiamo vedere **pick()**:\n",
    "```python\n",
    "def pick(states, parent_turn):\n",
    "    \"\"\"\n",
    "    Picks the best state based on the heuristic values.\n",
    "\n",
    "    This function evaluates a list of game states and selects the state that optimizes\n",
    "    the current player's position.\n",
    "    If it is the maximizing player's turn (parent_turn is True), the state with the highest heuristic\n",
    "    value is chosen.\n",
    "    Otherwise, if it is the minimizing player's turn (parent_turn is False), the state with the lowest heuristic\n",
    "    value is chosen.\n",
    "\n",
    "    :param states: List of game states to pick from.\n",
    "    :param parent_turn: Indicates whose turn it is: True for maximizing player and False for minimizing player.\n",
    "    :return: The best state based on the heuristic value.\n",
    "    \"\"\"\n",
    "    if parent_turn:\n",
    "        # If it's the maximizing player's turn, select the state with the highest heuristic value.\n",
    "        return max(states, key=lambda state: state.h)\n",
    "    else:\n",
    "        # If it's the minimizing player's turn, select the state with the lowest heuristic value.\n",
    "        return min(states, key=lambda state: state.h)\n",
    "```\n",
    "La funzione pick() restituisce, in base al turno (True per giocatore 1 e False per giocatore 2), lo stato con valore estimate massimo o minimo tra gli stati neighbors per ogni mossa. È uguale alla funzione che troviamo nel MinMax\n",
    "\n",
    "Di seguito possiamo vedere **evaluate()**:\n",
    "```python\n",
    "def evaluate(self, states, parent_turn):\n",
    "    \"\"\"\n",
    "    Evaluates a list of states and updates their heuristic values.\n",
    "\n",
    "    :param states: A list of game states to evaluate.\n",
    "    :param parent_turn: A flag indicating if it's the parent player's turn.\n",
    "    \"\"\"\n",
    "    for state in states:\n",
    "        # If a draw can be claimed in the current state, set heuristic value to 0.0.\n",
    "        if state.game_board.can_claim_draw():\n",
    "            state.h = 0.0\n",
    "        else:\n",
    "            # Otherwise, evaluate the state using the Minimax algorithm with Alpha-Beta pruning.\n",
    "            state.h = self.__minmax_alpha_beta(state, self.max_depth - 1, float(\"-inf\"), float(\"inf\"),\n",
    "                                               not parent_turn)\n",
    "```\n",
    "La funzione evaluate() di MinMaxAlphaBetaPruning ha in più rispetto all’algoritmo MinMax le due variabili alpha e beta.\n",
    "\n",
    "Di seguito possiamo vedere la funzione helper **__minmax_alpha_beta()**:\n",
    "```python\n",
    "def __minmax_alpha_beta(self, state, depth, alpha, beta, turn):\n",
    "    \"\"\"\n",
    "    Private method implementing the Minimax algorithm with Alpha-Beta pruning.\n",
    "\n",
    "    :param state: The current game state.\n",
    "    :param depth: The current depth in the game tree.\n",
    "    :param alpha: The alpha value for Alpha-Beta pruning.\n",
    "    :param beta: The beta value for Alpha-Beta pruning.\n",
    "    :param turn: Flag indicating if it's the maximizing player's turn.\n",
    "    :return: The heuristic value of the state.\n",
    "    \"\"\"\n",
    "    self.eval_count += 1\n",
    "\n",
    "    # Base case: if maximum depth is reached or the game is over, return the heuristic value of the state.\n",
    "    if depth == 0 or state.game_board.is_game_over():\n",
    "        return self.heuristic.h(state)\n",
    "\n",
    "    # Generate all possible moves (neighbors) from the current state.\n",
    "    neighbors = self.game.neighbors(state)\n",
    "\n",
    "    if turn:  # If it's the maximizing player's turn.\n",
    "        value = float(\"-inf\")\n",
    "        for neighbor in neighbors:\n",
    "            # Recursively call the function to evaluate the neighbor state, updating the value and alpha.\n",
    "            value = max(value, self.__minmax_alpha_beta(neighbor, depth - 1, alpha, beta, False))\n",
    "            alpha = max(alpha, value)\n",
    "            # Alpha-Beta pruning: if alpha is greater or equal to beta, prune this branch.\n",
    "            if alpha >= beta:\n",
    "                self.prune_count += 1\n",
    "                break\n",
    "        return value\n",
    "    else:  # If it's the minimizing player's turn.\n",
    "        value = float(\"inf\")\n",
    "        for neighbor in neighbors:\n",
    "            # Similarly, for the minimizing player, update the value and beta.\n",
    "            value = min(value, self.__minmax_alpha_beta(neighbor, depth - 1, alpha, beta, True))\n",
    "            beta = min(beta, value)\n",
    "            # Alpha-Beta pruning: if beta is less or equal to alpha, prune this branch.\n",
    "            if beta <= alpha:\n",
    "                self.prune_count += 1\n",
    "                break\n",
    "        return value\n",
    "```\n",
    "Nella funzione helper **__minmax_alpha_beta()** dell'algoritmo MinMaxAlphaBetaPruning, viene integrata la fase di \"pruning\", che verifica la convenienza di uno stato. Se questo stato risulta non vantaggioso, l'analisi del ramo corrispondente viene interrotta. Le variabili *eval_count* e *prune_count* servono rispettivamente a monitorare il numero di stati esaminati e il numero di potature realizzate."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f19756654010c5d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MinMaxAlpha-BetaPruning H0 CutOff\n",
    "\n",
    "Questa versione di MinMax con Alpha-Beta Pruning è una delle tre versioni che ottimizzano i tempi di valutazione e che ha lo scopo di vedere in profondità in un breve periodo di tempo.\n",
    "\n",
    "Il suo obiettivo principale è ridurre il numero di nodi valutati nell'albero di ricerca, \"tagliando\" i primi rami usando una valutazione statica con un euristica H0, che nel mio caso è l'euristica SoftBoardEvaluationChessGame.\n",
    "\n",
    "Questo permette di esplorare alberi più profondi in meno tempo, migliorando le prestazioni di \"scoperta\".\n",
    "\n",
    "L’algoritmo per essere istanziato ha bisogno dell’euristica di evaluation, dell’euristica di taglio h0, del gioco e della profondità alla quale deve lavorare. La variabile *eval_count* è una variabile che conta il numero degli stati valutati dal semplice minmax alpha beta utile per stampare i risultati e la variabile *prune_count* è una variabile che ci dice quanti elementi sono stati potati dal semplice minmax alpha beta, abbiamo poi anche i valori *eval_h0_cut_count* e *eval_h0_cut_count* che svolgono la stessa funzione ma sono riferiti al processo h0.\n",
    "\n",
    "È importante notare che è stato introdotto un dizionario contenente le valutazioni già effettuate. Questo significa che se ci troviamo di fronte a uno stato e una profondità già calcolati in precedenza, eviteremo di ricalcolare la valutazione, ottimizzando così il processo.\n",
    "\n",
    "```python\n",
    "def __init__(self, game, heuristic, h0_cut, k=5, max_depth=1):\n",
    "    \"\"\"\n",
    "    Initializes the MinMaxAlphaBetaPruningH0Cut class with game settings, heuristics, and search parameters.\n",
    "\n",
    "    :param game: The current state of the chess game.\n",
    "    :param heuristic: Main heuristic function used for evaluating game states.\n",
    "    :param h0_cut: Secondary heuristic function used for h0 cutoff.\n",
    "    :param k: Number of states to consider after applying the h0 cutoff. Defaults to 5.\n",
    "    :param max_depth: Maximum depth for the Minimax search. Defaults to 1.\n",
    "    \"\"\"\n",
    "    self.game = game  # The current state of the chess game.\n",
    "    self.heuristic = heuristic  # Main heuristic function used to evaluate game states.\n",
    "    self.h0_cut = h0_cut  # Secondary heuristic used for the h0 cutoff.\n",
    "    self.k = k  # Number of states to consider after applying the h0 cutoff.\n",
    "    self.max_depth = max_depth  # Maximum depth for the Minimax search.\n",
    "    self.prune_count = 0  # Count of pruned branches in the main search.\n",
    "    self.eval_count = 0  # Count of evaluations in the main search.\n",
    "    self.eval_h0_cut_count = 0  # Count of evaluations for the h0 cutoff.\n",
    "    self.prune_h0_cut_count = 0  # Count of pruned branches due to the h0 cutoff.\n",
    "    self.memoization = {}  # Dictionary for storing previously calculated states.\n",
    "``` \n",
    "Come si può vedere dal file *AlessandroMattei_ChessGame.AIhw2.ipynb*, nel quale è contenuta l'intera implementazione, possiamo notare la presenza dei metodi **__minmax_alpha_beta()**, **__h0_cut()**, **evaluate()**, **pick()** e **search()**.\n",
    "\n",
    "Soffermiamoci a vedere solo le parti cambiate dal canonico MinMax Alpha-Beta. Gradiamo prima come è stato implementata la funzione **__h0_cut()**\n",
    "\n",
    "```python\n",
    "def __h0_cut(self, states, turn):\n",
    "    \"\"\"\n",
    "    Applies the h0 cutoff heuristic to limit the number of states considered.\n",
    "\n",
    "    :param states: A list of game states.\n",
    "    :param turn: Flag indicating the current player's turn.\n",
    "    :return: A list of states after applying the h0 cutoff.\n",
    "    \"\"\"\n",
    "    initial_count = len(states)\n",
    "    # Evaluate states using the h0 heuristic and count evaluations.\n",
    "    for state in states:\n",
    "        state.h0 = self.h0_cut.h(state)\n",
    "        self.eval_h0_cut_count += 1\n",
    "\n",
    "    # Sort and select the top k states based on the h0 heuristic value.\n",
    "    sorted_states = sorted(states, key=lambda state: state.h0, reverse=turn)[:self.k]\n",
    "    # Count how many states were pruned by this process.\n",
    "    self.prune_h0_cut_count += initial_count - len(sorted_states)\n",
    "\n",
    "    return sorted_states\n",
    "```\n",
    "Questo metodo è privato e applica la euristica h0 cutoff per limitare il numero di stati considerati durante la ricerca. Prende una lista di stati possibili **states** e una variabile **turn** che indica il turno del giocatore corrente. Per ciascuno degli stati nella lista, calcola un valore euristico h0 utilizzando la funzione **h0_cut.h(state)** e tiene traccia delle valutazioni tramite **eval_h0_cut_count**. Successivamente, ordina gli stati in base ai valori h0 in ordine decrescente (o crescente, a seconda del turno) e restituisce i primi k stati, dove k è il numero di stati da considerare dopo l'applicazione dell'h0 cutoff. Questo metodo tiene anche traccia del numero di stati che sono stati \"potati\" dalla lista iniziale tramite **prune_h0_cut_count**\n",
    "\n",
    "Guardiamo **__minmax_alpha_beta()**\n",
    "\n",
    "```python\n",
    "def __minmax_alpha_beta(self, state, depth, alpha, beta, turn):\n",
    "    \"\"\"\n",
    "    Private method implementing the Minimax algorithm with Alpha-Beta pruning and memoization.\n",
    "\n",
    "    :param state: The current game state.\n",
    "    :param depth: The current depth in the game tree.\n",
    "    :param alpha: The alpha value for Alpha-Beta pruning.\n",
    "    :param beta: The beta value for Alpha-Beta pruning.\n",
    "    :param turn: Flag indicating if it's the maximizing player's turn.\n",
    "    :return: The heuristic value of the state.\n",
    "    \"\"\"\n",
    "    self.eval_count += 1\n",
    "\n",
    "    # Check if the state is already evaluated and stored in memoization.\n",
    "    if (state, depth, turn) in self.memoization:\n",
    "        return self.memoization[(state, depth, turn)]\n",
    "\n",
    "    # Base case: if maximum depth is reached or the game is over, return the heuristic value.\n",
    "    if depth == 0 or state.game_board.is_game_over():\n",
    "        return self.heuristic.h(state)\n",
    "\n",
    "    # Generate possible moves (neighbors), applying the h0 cutoff.\n",
    "    neighbors = self.game.neighbors(state)\n",
    "    top_neighbors = self.__h0_cut(neighbors, state.game_board.turn)\n",
    "\n",
    "    if turn:  # Maximizing player's turn.\n",
    "        value = float(\"-inf\")\n",
    "        for neighbor in top_neighbors:\n",
    "            # Recursively evaluate the state, update value and alpha.\n",
    "            value = max(value, self.__minmax_alpha_beta(neighbor, depth - 1, alpha, beta, False))\n",
    "            alpha = max(alpha, value)\n",
    "            # Alpha-Beta pruning: prune if alpha >= beta.\n",
    "            if alpha >= beta:\n",
    "                self.prune_count += 1\n",
    "                break\n",
    "        self.memoization[(state, depth, turn)] = value\n",
    "        return value\n",
    "    else:  # Minimizing player's turn.\n",
    "        value = float(\"inf\")\n",
    "        for neighbor in top_neighbors:\n",
    "            # Similar evaluation for the minimizing player.\n",
    "            value = min(value, self.__minmax_alpha_beta(neighbor, depth - 1, alpha, beta, True))\n",
    "            beta = min(beta, value)\n",
    "            # Prune if beta <= alpha.\n",
    "            if beta <= alpha:\n",
    "                self.prune_count += 1\n",
    "                break\n",
    "        self.memoization[(state, depth, turn)] = value\n",
    "        return value\n",
    "```\n",
    "Questo metodo privato implementa l'algoritmo Minimax con potatura Alpha-Beta, con memorizzazione degli stati e depth valutati e utilizza __h0_cut per valutare ed esplorare solo gli stati più promettenti. Prende come argomenti lo stato corrente state, la profondità corrente nella ricerca depth, i valori alpha e beta per la potatura Alpha-Beta, e un flag turn che indica se è il turno del giocatore massimizzante. Questo metodo valuta ricorsivamente gli stati nel gioco, utilizzando la memorizzazione per evitare di valutare più volte gli stessi stati con la stessa depth. Applica la potatura Alpha-Beta per ridurre il numero di stati da considerare e calcola il valore euristico del miglior stato possibile. Questo metodo tiene traccia del numero di valutazioni effettuate tramite eval_count.\n",
    "\n",
    "Guardiamo **__search()**\n",
    "\n",
    "```python\n",
    "def search(self, state: StateChessGame):\n",
    "    \"\"\"\n",
    "    Public method to start the search with Alpha-Beta pruning and h0 cutoff.\n",
    "\n",
    "    :param state: The current state of the chess game.\n",
    "    :return: The best next state for the current player.\n",
    "    \"\"\"\n",
    "    # Generate possible moves, applying the h0 cutoff.\n",
    "    neighbors = self.game.neighbors(state)\n",
    "    top_neighbors = self.__h0_cut(neighbors, state.game_board.turn)\n",
    "    # Evaluate the top neighbors and choose the best move based on the player's turn.\n",
    "    self.evaluate(top_neighbors, state.game_board.turn)\n",
    "    return self.pick(top_neighbors, state.game_board.turn)\n",
    "```\n",
    "Questo metodo pubblico avvia la ricerca utilizzando l'algoritmo Minimax con potatura Alpha-Beta e l'h0 cutoff. Prende uno stato state come input, genera mosse possibili applicando l'h0 cutoff, quindi valuta queste mosse e restituisce la migliore mossa possibile in base al turno del giocatore corrente. La valutazione viene effettuata utilizzando il metodo evaluate, e la scelta della mossa migliore viene fatta utilizzando il metodo pick.\n",
    "\n",
    "I restanti metodi non sono cambiati."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "76112655360cf40a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MinMaxAlpha-BetaPruning Hl CutOff\n",
    "\n",
    "Questa versione di MinMax con Alpha-Beta Pruning è una delle tre versioni che ottimizzano i tempi di valutazione e che ha lo scopo di vedere in profondità in un breve periodo di tempo.\n",
    "\n",
    "Il suo obiettivo principale è ridurre il numero di nodi valutati nell'albero di ricerca, \"tagliando\" i primi rami usando una valutazione dinamica hl cioè valutando i primi stati in profondità l, vine anche usato il \"taglio\" h0 che usa l'euristica SoftBoardEvaluationChessGame all'interno delle depth del minmax.\n",
    "\n",
    "Questo permette di esplorare alberi più profondi in meno tempo, migliorando le prestazioni di \"scoperta\".\n",
    "\n",
    "L’algoritmo per essere istanziato ha bisogno dell’euristica di evaluation, dell’euristica di taglio h0, del gioco e della profondità alla quale deve lavorare. La variabile *eval_count* è una variabile che conta il numero degli stati valutati dal semplice minmax alpha beta utile per stampare i risultati e la variabile *prune_count* è una variabile che ci dice quanti elementi sono stati potati dal semplice minmax alpha beta, abbiamo poi anche i valori *eval_h0_cut_count* e *eval_h0_cut_count* che svolgono la stessa funzione ma sono riferiti al processo h0 e *eval_hl_cut_count* e *eval_hl_cut_count* al processo hl.\n",
    "\n",
    "È importante notare che è stato introdotto un dizionario contenente le valutazioni già effettuate. Questo significa che se ci troviamo di fronte a uno stato e una profondità già calcolati in precedenza, eviteremo di ricalcolare la valutazione, ottimizzando così il processo.\n",
    "\n",
    "```python\n",
    "def __init__(self, game, heuristic, h0_cut, k=5, l=3, max_depth=1):\n",
    "    \"\"\"\n",
    "    Initializes the MinMaxAlphaBetaPruningHlCut class with game settings, heuristics, and search parameters.\n",
    "\n",
    "    :param game: The current state of the chess game.\n",
    "    :param heuristic: Main heuristic function used for evaluating game states.\n",
    "    :param h0_cut: Heuristic function used for the h0 cutoff.\n",
    "    :param k: Number of states to consider after applying the h0 and hl cutoffs. Defaults to 5.\n",
    "    :param l: Depth for the hl cutoff calculation. Defaults to 3.\n",
    "    :param max_depth: Maximum depth for the Minimax search. Defaults to 1.\n",
    "    \"\"\"\n",
    "    self.game = game  # The current state of the chess game.\n",
    "    self.heuristic = heuristic  # Main heuristic function for evaluating game states.\n",
    "    self.h0_cut = h0_cut  # Heuristic function used for the h0 cutoff.\n",
    "    self.k = k  # Number of states to consider after applying the h0 and hl cutoffs.\n",
    "    self.l = l  # Depth for the hl cutoff calculation.\n",
    "    self.max_depth = max_depth  # Maximum depth for the Minimax search.\n",
    "    self.prune_count = 0  # Count of pruned branches in the main search.\n",
    "    self.eval_count = 0  # Count of evaluations in the main search.\n",
    "    self.eval_h0_cut_count = 0  # Count of evaluations for the h0 cutoff.\n",
    "    self.prune_h0_cut_count = 0  # Count of pruned branches due to the h0 cutoff.\n",
    "    self.eval_hl_cut_count = 0  # Count of evaluations for the hl cutoff.\n",
    "    self.prune_hl_cut_count = 0  # Count of pruned branches due to the hl cutoff.\n",
    "    self.memoization = {}  # Dictionary for storing previously calculated states.\n",
    "```\n",
    "\n",
    "Come si può vedere dal file *AlessandroMattei_ChessGame.AIhw2.ipynb*, nel quale è contenuta l'intera implementazione, possiamo notare la presenza dei metodi **__minmax_alpha_beta()**, **__h0_cut()**, **__hl_cut()**, **__minmax_alpha_beta_hl()**, **evaluate()**, **pick()** e **search()**.\n",
    "\n",
    "Soffermiamoci a vedere solo le parti cambiate dal canonico MinMax Alpha-Beta. Analizziamo prima la funzione helper **__hl_cut()**:\n",
    "```python\n",
    "def __hl_cut(self, states, turn):\n",
    "    \"\"\"\n",
    "    Applies the hl cutoff heuristic to further limit the number of states considered.\n",
    "\n",
    "    :param states: A list of game states.\n",
    "    :param turn: Flag indicating the current player's turn.\n",
    "    :return: A list of states after applying the hl cutoff.\n",
    "    \"\"\"\n",
    "    initial_count = len(states)\n",
    "    # Evaluate states using a deeper level of the Minimax algorithm (hl cutoff).\n",
    "    for state in states:\n",
    "        state.hl = self.__minmax_alpha_beta_hl(state, self.l - 1, float(\"-inf\"), float(\"inf\"), not turn)\n",
    "    # Sort and select the top k states based on the hl heuristic value.\n",
    "    sorted_states = sorted(states, key=lambda state: state.hl, reverse=turn)[:self.k]\n",
    "    # Count how many states were pruned by this process.\n",
    "    self.prune_hl_cut_count += initial_count - len(sorted_states)\n",
    "    return sorted_states\n",
    "```\n",
    "Questo è un metodo privato che applica l'euristica di taglio hl per limitare ulteriormente il numero di stati considerati durante la ricerca. Prende una lista di stati possibili states e una variabile turn che indica il turno del giocatore corrente. Per ciascuno degli stati nella lista, calcola un valore euristico hl utilizzando il metodo __minmax_alpha_beta_hl(state, depth, alpha, beta, not turn). Questo valore hl viene utilizzato per valutare e ordinare gli stati. Successivamente, restituisce i primi k stati in base ai valori hl (in ordine decrescente o crescente, a seconda del turno) e tiene traccia del numero di stati che sono stati \"potati\" dalla lista iniziale tramite prune_hl_cut_count.\n",
    "\n",
    "Gradiamo ora il metodo **__minmax_alpha_beta_hl()**:\n",
    "```python\n",
    "def __minmax_alpha_beta_hl(self, state, depth, alpha, beta, turn):\n",
    "    \"\"\"\n",
    "    Implements a deeper level of the Minimax algorithm for the hl cutoff.\n",
    "\n",
    "    :param state: The current game state.\n",
    "    :param depth: The current depth in the game tree.\n",
    "    :param alpha: The alpha value for Alpha-Beta pruning.\n",
    "    :param beta: The beta value for Alpha-Beta pruning.\n",
    "    :param turn: Flag indicating if it's the maximizing player's turn.\n",
    "    :return: The heuristic value of the state.\n",
    "    \"\"\"\n",
    "    self.eval_hl_cut_count += 1\n",
    "\n",
    "    # Base case: if maximum depth is reached or the game is over, return the heuristic value from h0_cut.\n",
    "    if depth == 0 or state.game_board.is_game_over():\n",
    "        return self.h0_cut.h(state)\n",
    "\n",
    "    neighbors = self.game.neighbors(state)\n",
    "\n",
    "    if turn:  # Maximizing player's turn.\n",
    "        value = float(\"-inf\")\n",
    "        for neighbor in neighbors:\n",
    "            # Recursively evaluate the state for hl cutoff, update value and alpha.\n",
    "            value = max(value, self.__minmax_alpha_beta_hl(neighbor, depth - 1, alpha, beta, False))\n",
    "            alpha = max(alpha, value)\n",
    "            # Alpha-Beta pruning for hl cutoff.\n",
    "            if alpha >= beta:\n",
    "                self.prune_hl_cut_count += 1\n",
    "                break\n",
    "        return value\n",
    "    else:  # Minimizing player's turn.\n",
    "        value = float(\"inf\")\n",
    "        for neighbor in neighbors:\n",
    "            # Similar evaluation for the minimizing player for hl cutoff.\n",
    "            value = min(value, self.__minmax_alpha_beta_hl(neighbor, depth - 1, alpha, beta, True))\n",
    "            beta = min(beta, value)\n",
    "            # Prune if beta <= alpha in hl cutoff.\n",
    "            if beta <= alpha:\n",
    "                self.prune_hl_cut_count += 1\n",
    "                break\n",
    "        return value\n",
    "```\n",
    "Questo è un metodo privato che implementa una versione più profonda dell'algoritmo Minimax con potatura Alpha-Beta per il taglio hl. Prende come argomenti lo stato corrente state, la profondità corrente nella ricerca depth, i valori alpha e beta per la potatura Alpha-Beta, e un flag turn che indica se è il turno del giocatore massimizzante. Questo metodo valuta ricorsivamente gli stati nel gioco utilizzando la profondità specificata l e calcola il valore euristico del miglior stato possibile. Questo metodo tiene traccia del numero di valutazioni effettuate tramite eval_hl_cut_count.\n",
    "\n",
    "\n",
    "Analizziamo il metodo **__h0_cut()**:\n",
    "\n",
    "```python\n",
    "def __h0_cut(self, states, turn):\n",
    "    \"\"\"\n",
    "    Applies the h0 cutoff heuristic to limit the number of states considered.\n",
    "\n",
    "    :param states: A list of game states.\n",
    "    :param turn: Flag indicating the current player's turn.\n",
    "    :return: A list of states after applying the h0 cutoff.\n",
    "    \"\"\"\n",
    "    initial_count = len(states)\n",
    "    # Evaluate states using the h0 heuristic and count evaluations.\n",
    "    for state in states:\n",
    "        state.h0 = self.h0_cut.h(state)\n",
    "        self.eval_h0_cut_count += 1\n",
    "\n",
    "    # Sort and select the top k states based on the h0 heuristic value.\n",
    "    sorted_states = sorted(states, key=lambda state: state.h0, reverse=turn)[:self.k]\n",
    "    # Count how many states were pruned by this process.\n",
    "    self.prune_h0_cut_count += initial_count - len(sorted_states)\n",
    "\n",
    "    return sorted_states\n",
    "```\n",
    "Questo è un metodo privato che applica l'euristica di taglio h0 per limitare il numero di stati considerati durante la ricerca. Il suo funzionamento è simile al metodo __hl_cut, ma applica l'euristica h0 invece di hl e tiene traccia del numero di stati \"potati\" tramite prune_h0_cut_count.\n",
    "\n",
    "Guardiamo **__minmax_alpha_beta()**\n",
    "\n",
    "```python\n",
    "def __minmax_alpha_beta(self, state, depth, alpha, beta, turn):\n",
    "    \"\"\"\n",
    "    Private method implementing the Minimax algorithm with Alpha-Beta pruning and memoization.\n",
    "\n",
    "    :param state: The current game state.\n",
    "    :param depth: The current depth in the game tree.\n",
    "    :param alpha: The alpha value for Alpha-Beta pruning.\n",
    "    :param beta: The beta value for Alpha-Beta pruning.\n",
    "    :param turn: Flag indicating if it's the maximizing player's turn.\n",
    "    :return: The heuristic value of the state.\n",
    "    \"\"\"\n",
    "    self.eval_count += 1\n",
    "\n",
    "    # Check if the state is already evaluated and stored in memoization.\n",
    "    if (state, depth, turn) in self.memoization:\n",
    "        return self.memoization[(state, depth, turn)]\n",
    "\n",
    "    # Base case: if maximum depth is reached or the game is over, return the heuristic value.\n",
    "    if depth == 0 or state.game_board.is_game_over():\n",
    "        return self.heuristic.h(state)\n",
    "\n",
    "    # Generate possible moves (neighbors), applying the h0 cutoff.\n",
    "    neighbors = self.game.neighbors(state)\n",
    "    top_neighbors = self.__h0_cut(neighbors, state.game_board.turn)\n",
    "\n",
    "    if turn:  # Maximizing player's turn.\n",
    "        value = float(\"-inf\")\n",
    "        for neighbor in top_neighbors:\n",
    "            # Recursively evaluate the state, update value and alpha.\n",
    "            value = max(value, self.__minmax_alpha_beta(neighbor, depth - 1, alpha, beta, False))\n",
    "            alpha = max(alpha, value)\n",
    "            # Alpha-Beta pruning: prune if alpha >= beta.\n",
    "            if alpha >= beta:\n",
    "                self.prune_count += 1\n",
    "                break\n",
    "        self.memoization[(state, depth, turn)] = value\n",
    "        return value\n",
    "    else:  # Minimizing player's turn.\n",
    "        value = float(\"inf\")\n",
    "        for neighbor in top_neighbors:\n",
    "            # Similar evaluation for the minimizing player.\n",
    "            value = min(value, self.__minmax_alpha_beta(neighbor, depth - 1, alpha, beta, True))\n",
    "            beta = min(beta, value)\n",
    "            # Prune if beta <= alpha.\n",
    "            if beta <= alpha:\n",
    "                self.prune_count += 1\n",
    "                break\n",
    "        self.memoization[(state, depth, turn)] = value\n",
    "        return value\n",
    "```\n",
    "Questo metodo privato implementa l'algoritmo Minimax con potatura Alpha-Beta, con memorizzazione degli stati e depth valutati e utilizza __hl_cut e __h0_cut per valutare ed esplorare solo gli stati più promettenti. Prende come argomenti lo stato corrente state, la profondità corrente nella ricerca depth, i valori alpha e beta per la potatura Alpha-Beta, e un flag turn che indica se è il turno del giocatore massimizzante. Questo metodo valuta ricorsivamente gli stati nel gioco, utilizzando la memorizzazione per evitare di valutare più volte gli stessi stati con la stessa depth. Applica la potatura Alpha-Beta per ridurre il numero di stati da considerare e calcola il valore euristico del miglior stato possibile. Questo metodo tiene traccia del numero di valutazioni effettuate tramite eval_count.\n",
    "\n",
    "Guardiamo **__search()**\n",
    "\n",
    "```python\n",
    "def search(self, state: StateChessGame):\n",
    "    \"\"\"\n",
    "    Public method to start the search with Alpha-Beta pruning, h0, and hl cutoffs.\n",
    "\n",
    "    :param state: The current state of the chess game.\n",
    "    :return: The best next state for the current player.\n",
    "    \"\"\"\n",
    "    # Generate possible moves, applying the hl cutoff.\n",
    "    neighbors = self.game.neighbors(state)\n",
    "    top_neighbors = self.__hl_cut(neighbors, state.game_board.turn)\n",
    "    # Evaluate the top neighbors and choose the best move based on the player's turn.\n",
    "    self.evaluate(top_neighbors, state.game_board.turn)\n",
    "    return self.pick(top_neighbors, state.game_board.turn)\n",
    "```\n",
    "Questo è un metodo pubblico che avvia la ricerca utilizzando l'algoritmo Minimax con potatura Alpha-Beta, insieme alle euristiche di taglio hl e poi h0. Prende uno stato state come input, genera mosse possibili applicando il taglio hl, quindi valuta queste mosse e restituisce la migliore mossa possibile in base al turno del giocatore corrente. La valutazione viene effettuata utilizzando il metodo evaluate, e la scelta della mossa migliore viene fatta utilizzando il metodo pick.\n",
    "\n",
    "I restanti metodi non sono cambiati."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44c75be234100c43"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MinMaxAlpha-BetaPruning Hr CutOff (MLPRegressor)\n",
    "\n",
    "Questa versione di MinMax con Alpha-Beta Pruning è una delle tre versioni che ottimizzano i tempi di valutazione e che ha lo scopo di vedere in profondità in un breve periodo di tempo.\n",
    "\n",
    "Il suo obiettivo principale è ridurre il numero di nodi valutati nell'albero di ricerca, \"tagliando\" i primi rami usando un Regressore non-lineare per stabilire quali stati andranno esplorati.\n",
    "\n",
    "Questo permette di esplorare alberi più profondi in meno tempo, migliorando le prestazioni di \"scoperta\".\n",
    "\n",
    "L’algoritmo per essere istanziato ha bisogno dell’euristica di evaluation, dell’euristica di taglio h0, del gioco e della profondità alla quale deve lavorare. La variabile *eval_count* è una variabile che conta il numero degli stati valutati dal semplice minmax alpha beta utile per stampare i risultati e la variabile *prune_count* è una variabile che ci dice quanti elementi sono stati potati dal semplice minmax alpha beta, abbiamo poi anche i valori *eval_hr_cut_count* e *eval_hr_cut_count* che svolgono la stessa funzione ma sono riferiti al processo hr. Una nota importante è obbligatorio aver creato un modello prima di eseguire questo MinMax Alpha-Beta.\n",
    "\n",
    "È importante notare che è stato introdotto un dizionario contenente le valutazioni già effettuate. Questo significa che se ci troviamo di fronte a uno stato e una profondità già calcolati in precedenza, eviteremo di ricalcolare la valutazione, ottimizzando così il processo.\n",
    "\n",
    "```python\n",
    "def __init__(self, game, heuristic, k=5, max_depth=1):\n",
    "    \"\"\"\n",
    "    Initializes the MinMaxAlphaBetaPruningHrCut class with game settings, heuristics, and search parameters.\n",
    "\n",
    "    :param game: The current state of the chess game.\n",
    "    :param heuristic: Main heuristic function used for evaluating game states.\n",
    "    :param k: Number of states to consider after applying the hr cutoff. Defaults to 5.\n",
    "    :param max_depth: Maximum depth for the Minimax search. Defaults to 1.\n",
    "    \"\"\"\n",
    "    self.game = game  # The current state of the chess game.\n",
    "    self.heuristic = heuristic  # Main heuristic function used to evaluate game states.\n",
    "    self.k = k  # Number of states to consider after applying the h0 cutoff.\n",
    "    self.max_depth = max_depth  # Maximum depth for the Minimax search.\n",
    "    self.prune_count = 0  # Count of pruned branches in the main search.\n",
    "    self.eval_count = 0  # Count of evaluations in the main search.\n",
    "    self.eval_hr_cut_count = 0  # Count of evaluations for the h0 cutoff.\n",
    "    self.prune_hr_cut_count = 0  # Count of pruned branches due to the h0 cutoff.\n",
    "    self.memoization = {}  # Dictionary for storing previously calculated states.\n",
    "    self.mlp_regressor = joblib.load('./mlp_regressor_model.joblib')  # Load the ML regressor model.\n",
    "    self.observation = ObservationBoard(normalize_result=True)  # Initialize the observation board.\n",
    "```\n",
    "Come si può vedere dal file *AlessandroMattei_ChessGame.AIhw2.ipynb*, nel quale è contenuta l'intera implementazione, possiamo notare la presenza dei metodi **__minmax_alpha_beta()**, **__hr_cut()**, **__regressor_eval()**, **evaluate()**, **pick()** e **search()**.\n",
    "\n",
    "Soffermiamoci a vedere solo le parti cambiate dal canonico MinMax Alpha-Beta. Analizziamo prima la funzione helper **__hr_cut()**:\n",
    "```python\n",
    "def __hr_cut(self, states, turn):\n",
    "    \"\"\"\n",
    "    Applies the hr cutoff using the ML regressor to limit the number of states considered.\n",
    "\n",
    "    :param states: A list of game states.\n",
    "    :param turn: Flag indicating the current player's turn.\n",
    "    :return: A list of states after applying the hr cutoff.\n",
    "    \"\"\"\n",
    "    initial_count = len(states)\n",
    "\n",
    "    for state in states:\n",
    "        observations = self.observation.h_piccoli(state.game_board)  # Get observations from the board.\n",
    "        state.hr = self.__regressor_eval(observations)  # Evaluate state using the ML regressor.\n",
    "        self.eval_hr_cut_count += 1\n",
    "\n",
    "    # Sort and select the top k states based on the hr value.\n",
    "    sorted_states = sorted(states, key=lambda state: state.hr, reverse=turn)[:self.k]\n",
    "    # Count how many states were pruned by this process.\n",
    "    self.prune_hr_cut_count += initial_count - len(sorted_states)\n",
    "\n",
    "    return sorted_states\n",
    "```\n",
    "Questo è un metodo privato che applica l'euristica di taglio hr utilizzando un modello di regressione di machine learning per limitare il numero di stati considerati durante la ricerca. Prende una lista di stati possibili states e una variabile turn che indica il turno del giocatore corrente. Per ciascuno degli stati nella lista, estrae le osservazioni dalla scacchiera utilizzando l'oggetto ObservationBoard e quindi valuta lo stato utilizzando il metodo __regressor_eval(observations) per ottenere un valore hr. Successivamente, restituisce i primi k stati in base ai valori hr (in ordine decrescente o crescente, a seconda del turno) e tiene traccia del numero di stati che sono stati \"potati\" dalla lista iniziale tramite prune_hr_cut_count.\n",
    "\n",
    "Guardiamo **__regressor_eval()**\n",
    "```python\n",
    "def __regressor_eval(self, observations):\n",
    "    \"\"\"\n",
    "    Evaluates a state using the ML regressor.\n",
    "\n",
    "    :param observations: The observations extracted from the chess board.\n",
    "    :return: The predicted value from the ML regressor.\n",
    "    \"\"\"\n",
    "    colonne = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'h7', 'h8', 'h9', 'h10',\n",
    "               'h11', 'h12', 'h13', 'h14', 'h15', 'h16', 'h17', 'h18', 'h19',\n",
    "               'h20']\n",
    "    df = pd.DataFrame([observations], columns=colonne)\n",
    "    return self.mlp_regressor.predict(df)[0]  # Predict and return the first value.\n",
    "```\n",
    "Questo è un metodo privato che valuta uno stato utilizzando un modello di regressione di machine learning (ML). Prende le osservazioni estratte dalla scacchiera come input e restituisce il valore previsto dal modello di regressione per quel particolare stato.\n",
    "\n",
    "Guardiamo **__minmax_alpha_beta()**\n",
    "```python\n",
    "def __minmax_alpha_beta(self, state, depth, alpha, beta, turn):\n",
    "    \"\"\"\n",
    "    Private method implementing the Minimax algorithm with Alpha-Beta pruning.\n",
    "\n",
    "    :param state: The current game state.\n",
    "    :param depth: The current depth in the game tree.\n",
    "    :param alpha: The alpha value for Alpha-Beta pruning.\n",
    "    :param beta: The beta value for Alpha-Beta pruning.\n",
    "    :param turn: Flag indicating if it's the maximizing player's turn.\n",
    "    :return: The heuristic value of the state.\n",
    "    \"\"\"\n",
    "    self.eval_count += 1\n",
    "\n",
    "    # Check if the state is already evaluated and stored in memoization.\n",
    "    if (state, depth, turn) in self.memoization:\n",
    "        return self.memoization[(state, depth, turn)]\n",
    "\n",
    "    # Base case: if maximum depth is reached or the game is over, return the heuristic value.\n",
    "    if depth == 0 or state.game_board.is_game_over():\n",
    "        return self.heuristic.h(state)\n",
    "\n",
    "    # Generate possible moves (neighbors), applying the hr cutoff.\n",
    "    neighbors = self.game.neighbors(state)\n",
    "    top_neighbors = self.__hr_cut(neighbors, state.game_board.turn)\n",
    "\n",
    "    if turn:  # Maximizing player's turn.\n",
    "        value = float(\"-inf\")\n",
    "        for neighbor in top_neighbors:\n",
    "            # Recursively evaluate the state, update value and alpha.\n",
    "            value = max(value, self.__minmax_alpha_beta(neighbor, depth - 1, alpha, beta, False))\n",
    "            alpha = max(alpha, value)\n",
    "            # Alpha-Beta pruning: prune if alpha >= beta.\n",
    "            if alpha >= beta:\n",
    "                self.prune_count += 1\n",
    "                break\n",
    "        self.memoization[(state, depth, turn)] = value\n",
    "        return value\n",
    "    else:  # Minimizing player's turn.\n",
    "        value = float(\"inf\")\n",
    "        for neighbor in top_neighbors:\n",
    "            # Similar evaluation for the minimizing player.\n",
    "            value = min(value, self.__minmax_alpha_beta(neighbor, depth - 1, alpha, beta, True))\n",
    "            beta = min(beta, value)\n",
    "            # Prune if beta <= alpha.\n",
    "            if beta <= alpha:\n",
    "                self.prune_count += 1\n",
    "                break\n",
    "        self.memoization[(state, depth, turn)] = value\n",
    "        return value\n",
    "```\n",
    "Questo metodo privato implementa l'algoritmo Minimax con potatura Alpha-Beta, con memorizzazione degli stati e depth valutati e utilizza __hr_cut (un MPLRegressor) per valutare ed esplorare solo gli stati più promettenti. Prende come argomenti lo stato corrente state, la profondità corrente nella ricerca depth, i valori alpha e beta per la potatura Alpha-Beta, e un flag turn che indica se è il turno del giocatore massimizzante. Questo metodo valuta ricorsivamente gli stati nel gioco, utilizzando la memorizzazione per evitare di valutare più volte gli stessi stati con la stessa depth. Applica la potatura Alpha-Beta per ridurre il numero di stati da considerare e calcola il valore euristico del miglior stato possibile. Questo metodo tiene traccia del numero di valutazioni effettuate tramite eval_count.\n",
    "\n",
    "Guardiamo **__search()**\n",
    "```python\n",
    "def search(self, state: StateChessGame):\n",
    "    \"\"\"\n",
    "    Public method to start the search with Alpha-Beta pruning and hr cutoff.\n",
    "\n",
    "    :param state: The current state of the chess game.\n",
    "    :return: The best next state for the current player.\n",
    "    \"\"\"\n",
    "    # Generate possible moves, applying the h0 cutoff.\n",
    "    neighbors = self.game.neighbors(state)\n",
    "    top_neighbors = self.__hr_cut(neighbors, state.game_board.turn)\n",
    "    # Evaluate the top neighbors and choose the best move based on the player's turn.\n",
    "    self.evaluate(top_neighbors, state.game_board.turn)\n",
    "    return self.pick(top_neighbors, state.game_board.turn)\n",
    "```\n",
    "Questo è un metodo pubblico che avvia la ricerca utilizzando l'algoritmo Minimax con potatura Alpha-Beta, insieme all'euristica di taglio hr. Prende uno stato state come input, genera mosse possibili applicando il taglio hr, quindi valuta queste mosse e restituisce la migliore mossa possibile in base al turno del giocatore corrente. La valutazione viene effettuata utilizzando il metodo evaluate, e la scelta della mossa migliore viene fatta utilizzando il metodo pick."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "191e3cd41e2f484f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# HEURISTICS\n",
    "## HardBoardEvaluationChessGame - Complex Chess Board Evaluation\n",
    "\n",
    "Questa euristica combina varie euristiche. È l'euristica più complessa che è presente nel gioco.\n",
    "Ritorna la somma dei risultati delle singole euristiche.\n",
    "\n",
    "Euristica Combinate:\n",
    "   - evaluate_board_without_king: Componente di valutazione che si concentra sulla scacchiera senza considerare la posizione del re.\n",
    "   - evaluate_central_control_score: Componente di valutazione incentrata sul controllo delle caselle centrali.\n",
    "   - evaluate_king_safety: Componente di valutazione che si concentra sulla sicurezza del Re.\n",
    "   - evaluate_mobility: Componente di valutazione incentrata sulla mobilità dei pezzi.\n",
    "   - evaluate_pawn_structure: Componente di valutazione che si concentra sulla struttura dei pedoni.\n",
    "   - evaluate_piece_positions: Componente di valutazione che si concentra sulle posizioni di tutti i pezzi tranne il re.\n",
    "\n",
    "Per capire e vedere nel dettaglio come sono state implementate le singole euristiche vedere il file *AlessandroMattei_ChessGame.AIhw2.ipynb*"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "683691ec33db3b69"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SoftBoardEvaluationChessGame - Simple Chess Board Evaluation\n",
    "\n",
    "Questa euristica combina varie euristiche. È l'euristica più semplice che è presente per il gioco Scacchi e viene usata per tagliare gli stati in h0 e hl.\n",
    "Ritorna la somma dei risultati delle singole euristiche\n",
    "\n",
    "Euristica Combinate:\n",
    "   - evaluate_board_without_king: Componente di valutazione che si concentra sulla scacchiera senza considerare la posizione del re.\n",
    "   - evaluate_central_control_score: Componente di valutazione incentrata sul controllo delle caselle centrali.\n",
    "   - evaluate_king_safety: Componente di valutazione che si concentra sulla sicurezza del Re.\n",
    "   - evaluate_piece_positions: Componente di valutazione che si concentra sulle posizioni di tutti i pezzi tranne il re.\n",
    "\n",
    "Per capire e vedere nel dettaglio come sono state implementate le singole euristiche vedere il file *AlessandroMattei_ChessGame.AIhw2.ipynb*"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cec7d9d78864aa50"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creazione di un Regressore per il MinMax Alpha Beta con Hr CutOff\n",
    "## Creazione del dataset\n",
    "\n",
    "Prima di creare un regressore da utilizzare per il MinMax Alpha Beta, ho deciso di scaricare un dataset collaudato disponibile su Kaggle: [Chess Evaluations](https://www.kaggle.com/datasets/ronakbadhe/chess-evaluations).\n",
    "\n",
    "Questo dataset contiene oltre 12 milioni di righe e due colonne principali:\n",
    "- La colonna \"FEN\" che identifica la posizione della scacchiera in formato FEN.\n",
    "- La colonna \"Evaluation\" che rappresenta il valore calcolato utilizzando Stockfish 11 con profondità 22.\n",
    "\n",
    "Per velocizzare la computazione e semplificare il processo di debug, ho suddiviso il dataset totale in 130 file.\n",
    "\n",
    "Per creare il file CSV, è stata sviluppata una classe dedicata in grado di generare valutazioni a partire da una specifica configurazione della scacchiera. In totale, sono state generate 20 valutazioni per ciascuna configurazione. \n",
    "Il nuovo generatore CSV produce un dataset con 21 colonne:\n",
    "- Le colonne h1, h2, h3, h4, h5, h6, h7, h8, h9, h10, h11, h12, h13, h14, h15, h16, h17, h18, h19, h20 identificano le singole osservazioni.\n",
    "- La colonna \"HL\" identifica il valore calcolato da Stockfish.\n",
    "\n",
    "Questo dataset sarà fondamentale per addestrare il nostro regressore per il MinMax Alpha Beta.\n",
    "\n",
    "Vediamo il generatore:\n",
    "```python\n",
    "import os\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "import chess\n",
    "import pandas as pd\n",
    "\n",
    "from chessgame.heuristics.ObservationBoard import ObservationBoard\n",
    "\n",
    "\n",
    "def eval_fen(csv_row):\n",
    "    fen = csv_row['FEN']\n",
    "    hl = csv_row['Evaluation']\n",
    "    observation = ObservationBoard(normalize_result=True)\n",
    "    evaluation = observation.h_piccoli(chess.Board(fen))\n",
    "    return evaluation + [hl]\n",
    "\n",
    "\n",
    "def generate_csv():\n",
    "    # Numero di lavoratori\n",
    "    number_of_workers = os.cpu_count()\n",
    "\n",
    "    heuristic_csv = pd.DataFrame(\n",
    "        columns=['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'h7', 'h8', 'h9', 'h10', 'h11', 'h12', 'h13', 'h14', 'h15', 'h16',\n",
    "                 'h17', 'h18', 'h19', 'h20', 'HL'])\n",
    "\n",
    "    csv_num_files = 130\n",
    "    # Percorso della cartella dove si trovano i file\n",
    "    directory = '../csv/chessdata'\n",
    "\n",
    "    for i in range(1, csv_num_files + 1):\n",
    "        csv_file = f\"{directory}/chessData_partizione_{i}.csv\"\n",
    "        df_chunk = pd.read_csv(csv_file)\n",
    "        print(f\"\\ncarico il csv: {csv_file}\")\n",
    "        # Processa il chunk\n",
    "        with ProcessPoolExecutor(max_workers=number_of_workers) as executor:\n",
    "            res = list(executor.map(eval_fen, df_chunk.to_dict('records')))\n",
    "\n",
    "        heuristic_csv = pd.concat([heuristic_csv, pd.DataFrame(res, columns=heuristic_csv.columns)])\n",
    "        print(\"file elaborato\")\n",
    "\n",
    "    print(f\"\\nelaborati tutti i {csv_num_files} file. Scrivo il csv finale\\n\")\n",
    "    # Salva il nuovo DataFrame in un file CSV\n",
    "    heuristic_csv.to_csv('../csv/eval_dataset.csv', index=False)\n",
    "    print(\"csv finale scritto\\n\")\n",
    "\n",
    "    # Mostra le prime righe del nuovo DataFrame\n",
    "    print(heuristic_csv.head())\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    generate_csv()\n",
    "```\n",
    "Per capire e vedere nel dettaglio la generazione delle osservazioni e su come sono state implementate vedere il file *AlessandroMattei_ChessGame.AIhw2.ipynb*\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc78d68850e28931"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Addestramento di un Regressore MLP per Hr con dati da CSV\n",
    "\n",
    "In questo processo, ho adottato un approccio per addestrare un regressore basato su una rete neurale MLP (Multilayer Perceptron) per stimare Hr. ho utilizzato i dati dall file CSV generato prima che contiene osservazioni della scacchiera e le corrispondenti valutazioni HL ottenute da Stockfish.\n",
    "\n",
    "```python\n",
    "import datetime\n",
    "\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "\n",
    "def train_regressor():\n",
    "    df = pd.read_csv('../csv/eval_dataset.csv')\n",
    "    # Separare le features e il target\n",
    "    X = df.drop('HL', axis=1)  # Features: h1 a h20\n",
    "    y = df['HL']  # Target: HL\n",
    "\n",
    "    # Divisione del dataset in set di addestramento e di validazione\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Creazione del modello MLPRegressor\n",
    "    mlp_regressor = MLPRegressor(hidden_layer_sizes=(100, 50),\n",
    "                                 activation='relu',\n",
    "                                 solver='adam',\n",
    "                                 alpha=0.0001,\n",
    "                                 learning_rate_init=0.001,\n",
    "                                 max_iter=500,\n",
    "                                 early_stopping=True,\n",
    "                                 validation_fraction=0.1,\n",
    "                                 n_iter_no_change=10,\n",
    "                                 random_state=42,\n",
    "                                 verbose=True)\n",
    "\n",
    "    # Addestramento del modello\n",
    "    mlp_regressor.fit(X_train, y_train)\n",
    "\n",
    "    # Valutazione del modello sul set di validazione\n",
    "    y_val_pred = mlp_regressor.predict(X_val)\n",
    "    mse = mean_squared_error(y_val, y_val_pred)\n",
    "    r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "    print(f\"Errore Quadratico Medio sul set di validazione: {mse}\")\n",
    "    print(f\"Coefficiente di determinazione (R²) sul set di validazione: {r2}\")\n",
    "\n",
    "    # Salvare il modello addestrato\n",
    "    joblib.dump(mlp_regressor, 'mlp_regressor_model_c_64.joblib')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start_time = datetime.datetime.now()\n",
    "    print(f\"Addestramento iniziato a: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "    train_regressor()\n",
    "\n",
    "    end_time = datetime.datetime.now()\n",
    "    print(f\"Addestramento terminato a: {end_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"Durata totale: {end_time - start_time}\")\n",
    "```\n",
    "In Dettaglio:\n",
    "Questo programma legge un file CSV contenente dati strutturati, tra cui caratteristiche (features) e un obiettivo da predire (target). Le caratteristiche vengono estratte dal dataset, mentre il target viene isolato. Il dataset viene quindi diviso in due parti: un set di addestramento e un set di validazione per valutare le prestazioni del modello.\n",
    "\n",
    "Un regressore basato su una rete neurale MLP viene creato con parametri specifici, tra cui le dimensioni dei livelli nascosti, la funzione di attivazione, il metodo di ottimizzazione e altri. Il modello viene quindi addestrato utilizzando il set di addestramento per imparare la relazione tra le caratteristiche e il target.\n",
    "\n",
    "Dopo l'addestramento, il modello effettua previsioni sul set di validazione e calcola due metriche di valutazione principali: l'Errore Quadratico Medio (MSE) e il Coefficiente di Determinazione (R²). Queste metriche forniscono informazioni sulle prestazioni del modello nel predire il target in base ai dati di input.\n",
    "\n",
    "Infine, il modello addestrato viene salvato su disco utilizzando la libreria \"joblib\", e vengono registrati l'orario di inizio e di fine dell'addestramento, insieme alla durata totale dell'operazione."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "398ec98ccf237c5c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ChessGame - Chess\n",
    "Il gioco degli scacchi, rinomato e antico, è uno degli esempi più pregevoli di strategia tra i giochi da tavolo.\n",
    "La partita si dispiega su una scacchiera composta da 64 caselle, disposte in un alternarsi di colori chiari e scuri (tipicamente bianche e nere). La scacchiera viene posizionata tra i contendenti in modo che la casella situata in basso a destra sia di colore chiaro.\n",
    "\n",
    "**Pezzi**\n",
    "Ogni giocatore inizia con 16 pezzi:\n",
    "- 1 Re: Si muove di una casella in qualsiasi direzione.\n",
    "- 1 Regina (o Dama): Si muove di qualsiasi numero di caselle in linea retta, sia in orizzontale, verticale, che diagonale.\n",
    "- 2 Torri: Si muovono in linea retta, ma solo in orizzontale o verticale.\n",
    "- 2 Cavalli: Si muovono in una forma a \"L\", ovvero due caselle in una direzione e una perpendicolare a quella.\n",
    "- 2 Alfieri: Si muovono di qualsiasi numero di caselle, ma solo in diagonale.\n",
    "- 8 Pedoni: Si muovono in avanti di una casella alla volta, con l'eccezione del primo movi\n",
    "\n",
    "### OBBIETTIVO\n",
    "L'obiettivo principale è mettere in \"scacco matto\" il re avversario, creando una condizione in cui il monarca è minacciato e non può evitare la cattura.\n",
    "Il gioco può finire in pareggio, o \"patta\", in diverse circostanze, come quando nessuno dei giocatori ha sufficienti pezzi per dare scacco matto, o se si verifica una posizione ripetuta tre volte.\n",
    "\n",
    "### MODALITÀ DI GIOCO\n",
    "Due agenti si sfidano spostando i pezzi sulla scacchiera, facendo un turno alla volta.\n",
    "\n",
    "#### CLASSE GIOCO\n",
    "All'interno del progetto per questo gioco troviamo una classe chiamata **ChessGame** che ha il compito di fornire i metodi per ottenere i vicini di un dato stato, che sono i possibili stati che possono essere raggiunti effettuando mosse valide dallo stato corrente.\n",
    "\n",
    "Di seguito possiamo vedere la funzione helper **neighbors()** presente nella classe **ChessGame**:\n",
    "\n",
    "```python\n",
    "def neighbors(self, state: StateChessGame):\n",
    "    \"\"\"\n",
    "    Determines the neighboring states of the provided chess game state.\n",
    "    :param state: The current state of the chess game.\n",
    "    :return: A list of neighboring states for the given state.\n",
    "    \"\"\"\n",
    "    neighbors = []\n",
    "\n",
    "    # Iterate through all legal moves and compute the resulting game state\n",
    "    for legal_move in state.game_representation.get_all_legal_moves():\n",
    "        representation = state.game_representation.make_a_move(legal_move)\n",
    "        neighbor = StateChessGame(game_representation=representation, state_parent=state,\n",
    "                                  move=legal_move)\n",
    "        neighbors.append(neighbor)\n",
    "    return neighbors\n",
    "```\n",
    "La funzione **neighbors()** restituisce gli stati adiacenti a quello fornito, rappresentando tutte le configurazioni possibili dei pezzi ottenibili mediante mosse legali, sfruttando la libreria \"chess\"."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "13a053f036acb87c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# STATES\n",
    "\n",
    "Lo stato costituisce una chiave essenziale nel contesto del gioco o del problema, fungendo da fotografia istantanea della sua configurazione in un dato momento.\n",
    "All'interno dello stato, troviamo la rappresentazione dettagliata del tavolo da gioco o del contesto problematico, un riferimento al suo stato precedente o \"state_parent\", nonché una serie di parametri e valori numerici che forniscono una valutazione qualitativa e quantitativa di tale stato, aiutando nella sua interpretazione e nelle decisioni successive.\n",
    "\n",
    "## StateChessGame - STATE Chess\n",
    "\n",
    "```python\n",
    "def __init__(self, game_board=None, state_parent=None, move=None):\n",
    "    \"\"\"\n",
    "    Initializes a new game state.\n",
    "\n",
    "    :param game_board: The current chess board configuration. If None, initializes a new chess board.\n",
    "    :param state_parent: The parent state from which this state is derived.\n",
    "    :param move: The move that led to this state.\n",
    "    \"\"\"\n",
    "    self.game_board = game_board  # The current chess board (chess.Board object).\n",
    "    self.parent_state = state_parent  # The parent state from which this state is derived.\n",
    "    self.move = move  # The move that led to this state.\n",
    "    self.h = None  # General heuristic value for the state.\n",
    "    self.h0 = None  # Heuristic value used for h0 cutoff.\n",
    "    self.hl = None  # Heuristic value used for hl cutoff.\n",
    "    self.hr = None  # Heuristic value used for nonlinear regressor cutoff.\n",
    "\n",
    "    # If no game board is provided, initialize a new chess board.\n",
    "    if self.game_board is None:\n",
    "        self.game_board = chess.Board()\n",
    "```\n",
    "Per maggiori dettagli vedere il file *AlessandroMattei_ChessGame.AIhw2.ipynb*\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe0914dc6313fdbd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Chess Game Report\n",
    "Vengono riportati i risultati dei test effettuati.\n",
    "\n",
    "I test sono stati effettuati su un mini-pc con le seguenti caratteristiche:\n",
    "\n",
    "- CPU Intel i7-12450H limitato in potenza a 50 Wat (Intel setta il limite di potenza a 125wat ma il pc in questione non supporta tale potenza)\n",
    "- Ram 35gb DDR4 3600Mhz"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ed06696ac8811db"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1cb5e3993460c7f8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
